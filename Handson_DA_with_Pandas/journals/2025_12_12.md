## Chapter 2 Pandas Data structure

For the most part, `pandas` objects use NumPy arrays for their internal data representations. However, for some data types, `pandas` builds upon NumPy to create its own [[array]]s.
If we need to ensure we get a specific type back, then it is recommended to use the [[array]] attribute or `to_numpy()` method, respectively, instead of `values`.
	- #### 1. What does "respectively" mean here?
	  
	  The word "respectively" tells you to match them in that specific order. In English grammar, "respectively" allows a writer to list two groups of items and tell you that the **first item in list A** matches the **first item in list B**, and the **second** matches the **second**. 
	  
	  In your text, the author is linking the **method you should use** to the **data type you want**.
		- **Item 1:** If you want a `pandas.array` $\rightarrow$ use the `.array` attribute.
		- **Item 2:** If you want a `numpy.array` $\rightarrow$ use the `.to_numpy()` method.
	- #### 2. Why not use  `.values` ?
	  collapsed:: true
	  
	  The attribute `.values` is ambiguous. When you use it, you don't always know exactly what you are going to get.
		- **The Old Way (`.values`):** It tries to give you a NumPy representation. However, Pandas has evolved to support data types that NumPy doesn't handle well (like timezones or "nullable integers"). If you call `.values` on these complex types, the result can sometimes be unpredictable or inefficient.
		- **The New Way (`.to_numpy()`):** This is explicit. You are telling Pandas: *"I don't care what format you use internally, please convert this to a NumPy array for me right now."*
		- **The New Way (`.array`):** This is also explicit. You are telling Pandas: *"Give me the actual array you are using to store this data, even if it's a special Pandas Extension Array."*
		  
		  **Analogy:**
			- **`.values`** is like saying "Give me the money." You might get cash, a check, or store credit depending on what the store prefers.
			- **`.to_numpy()`** is like saying "Give me exactly $20 in cash."
			- **`.array`** is like saying "Give me the wallet itself."
	-
	- #### 3. Why does the book use  `.values`  in the example?
	  
	  You will see `.values` in almost all older tutorials and books.
		- **Legacy:** `.values` has existed since the beginning of Pandas. `.to_numpy()` was only added in later versions (Pandas v0.24). The book likely uses it because it is "standard" legacy code that still works for simple data.
		- **Simplicity:** For simple data (like a column of standard integers or floats), `.values` and `.to_numpy()` do the exact same thing. The book example `place_index.values` likely contains simple data, so `.values` works fine.
	- ### [[Panadas]] Data Structure
		- pandas.Series
		  Can perform  arithmetic operations element-wise between arrays like array1+array2 , besides that pandas can adjust the index if the index of two series does not 100% match, it will sum the matched indices.
		  ```
		  pandas.Series(numpy.array, name="name")
		  ```
		- pandas.DataFrame
			- The `DataFrame` gives us a representation of a table formed from many `Series` objects that form the columns and a shared `Index` object that labels the rows.
			- We can create a `DataFrame` object from  NumPy representations
			  
			  ```
			  df = pandas.DataFrame(array_dict)
			  # array dict is like {col_name1:[arr],col_name2:[arr]}
			  df = pd.Series(numpy.array).to_frame()
			  
			  df = pd.DataFrame(
			      list_of_tuples, 
			      columns=['n', 'n_squared', 'n_cubed']
			  )
			  
			  df = pd.DataFrame(
			      np.array([
			          [0, 0, 0],
			          [1, 1, 1],
			          [2, 4, 8],
			          [3, 9, 27],
			          [4, 16, 64]
			      ]), columns=['n', 'n_squared', 'n_cubed']
			  )
			  
			  #read from data frame
			  df = pd.read_csv('data/earthquakes.csv')
			  # read from url
			  df = pd.read_csv('https://github.com/stefmolin/'
			      'Hands-On-Data-Analysis-with-Pandas-2nd-edition'
			      '/blob/master/ch_02/data/earthquakes.csv?raw=True'
			  )
			  
			  ```
			- we can also perform arithmetic on dataframes. Pandas will only perform the operation when both the index and column match.
			  ```
			  df+df
			  ```
			- More data frame basic check
			  ```
			  df.empty
			  df.shape
			  df.columns
			  df.head()
			  df.tail()
			  df.dtypes
			  df.info()
			  
			  # for describe 
			  df.describe() # for integer or float data type
			  df.describe(include = np.object) # specific data type colums 
			  df.describe(include = 'all')
			  
			  ## specify column
			  df.your_column_name.describe()
			  
			  ## df data add in and remove 
			  ### directly create
			  df['new_col'] = 'xxx' 
			  
			  ### create new depends on other columns 
			  df['new_col'] = df.old_col < 0  # bool mask
			  
			  ### filter results with condition logic 
			  np.where(df['A'] > 10, 'High', 'Low')
			  
			  ```
			- Row-wise String Splitting/Extraction
			  
			  When you apply str.extract() with a regex that includes one or more capturing 
			  groups, Pandas returns a DataFrame, even if you only have one capturing group.
			  If you have N capturing groups, the resulting DataFrame will have N columns.
			  ```
			  df.column_name.str.extract(r'(regx1)|(regex2)')
			  ```
			  have one capturing group: (.*$). Therefore, the output of 
			  df.place.str.extract(r', (.*$)') is a Pandas DataFrame with a single column. 
			  using slicing [0]  e.g.
			  ```
			  df.place.str.extract(r', (.*$)')[0].sort_values().unique()
			  ```
			  or using name group and select column by name
			  ```
			  df.place.extract(r', (?P<country>.*$)')['country'].sort_values().unique()
			  ```
			  
			  Extract multiple groups and combine extract group results into one series 
			  ```
			  extracted_df = df.column_name.str.extract(r'(regex1)|(regex2)') 
			  
			  # fills the NaN values in column 0 with the non-NaN values from column 1.
			  combined_series = extracted_df[0].fillna(extracted_df[1])
			  ```
			- Assign new column with logic and lambda 
			  add in new columns with logic or self defined lambda 
			  For the lambda function you can work on any columns in the df and the new col you are going to create 
			  ```
			  df.assign(
			      in_ca=df.parsed_place == 'California',
			      in_alaska=df.parsed_place == 'Alaska',
			      neither=lambda x: ~x.in_ca & ~x.in_alaska
			  ).sample(5, random_state=0)
			  
			  df.assign(
			      is_expensive_ca=lambda x: (x.parsed_place == 'California') & (x.price > 100)
			  )
			  ```
			  Assign with extract method
			  ```
			  new_cols = ['Type', 'Size', 'ColorCode']
			  df[new_cols] = df['ID'].str.extract(r'([A-Z]{2})-([0-9])([A-Z]{2})')
			  
			  
			  # create new column by loc one column and fill Na with another columns value 
			  extracted_df = df['column_name'].str.extract(r'(regex1)|(regex2)|(regex3)')
			  
			  # Consolidate column 0, then column 1, then column 2
			  df['Cleaned_Value'] = (
			      extracted_df.iloc[:, 0]
			      .fillna(extracted_df.iloc[:, 1])
			      .fillna(extracted_df.iloc[:, 2])
			  )
			  
			  ```
			- Mapping / replace values
			  ```
			  df['State'].map({'CA': 'California'})
			  
			  ## str data operation , replace
			  df['parsed_place'] = df.place.str.replace(
			      r'.* of ', '', regex=True # remove anything saying <something> of <something>
			  ).str.replace(
			      'the ', '' # remove "the "
			  ).str.replace(
			      r'CA$', 'California', regex=True ).str.strip() # fix California
			  
			  ```
			- Create new col with using np.select
			  ```
			  
			  df['col1] = np.select( condlist = [df['col2']==xx, df['col3']>=yy],
			                   choicelist = ['a','b'], # maping to the condition 
			                    defualt = 'default value' ) # for handling unexpected value 
			  ```
			- Create new
			- ```
			  filter_func = lambda frame: frame['xx'] >= YY 
			  # or more complex function but the imput is the dataframe and return a series with bool value
			  # Execute the function and pass the resulting Boolean Series to df.loc[]
			  df.loc[filter_func(df), 'col2'].std()
			  ```
			- #### Statistic method for data in table 
			  
			  ```
			  | Method | Description | Data types |
			  | --- | --- | --- |
			  | `count()` | The number of non-null observations | Any |
			  | `nunique()` | The number of unique values | Any |
			  | `sum()` | The total of the values | Numerical or Boolean |
			  | `mean()` | The average of the values | Numerical or Boolean |
			  | `median()` | The median of the values | Numerical |
			  | `min()` | The minimum of the values | Numerical |
			  | `idxmin()` | The index where the minimum values occurs | Numerical |
			  | `max()` | The maximum of the values | Numerical |
			  | `idxmax()` | The index where the maximum value occurs | Numerical |
			  | `abs()` | The absolute values of the data | Numerical |
			  | `std()` | The standard deviation | Numerical |
			  | `var()` | The variance |  Numerical |
			  | `cov()` | The covariance between two `Series`, or a covariance matrix for all column combinations in a `DataFrame` | Numerical |
			  | `corr()` | The correlation between two `Series`, or a correlation matrix for all column combinations in a `DataFrame` | Numerical |
			  | `quantile()` | Calculates a specific quantile | Numerical |
			  | `cumsum()` | The cumulative sum | Numerical or Boolean |
			  | `cummin()` | The cumulative minimum | Numerical |
			  | `cummax()` | The cumulative maximum | Numerical |
			  ```
			- #### Index methods for index 
			  
			  ```
			  | Method | Description |
			  | --- | --- |
			  | `argmax()`/`argmin()` | Find the location of the maximum/minimum value in the index |
			  | `equals()` | Compare the index to another `Index` object for equality |
			  | `isin()` | Check if the index values are in a list of values and return an array of Booleans |
			  | `max()`/`min()` | Find the maximum/minimum value in the index |
			  | `nunique()` | Get the number of unique values in the index |
			  | `to_series()` | Create a `Series` object from the index |
			  | `unique()` | Find the unique values of the index |
			  | `value_counts()`| Create a frequency table for the unique values in the index |
			  ```