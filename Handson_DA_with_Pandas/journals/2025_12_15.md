### Pandas  Data Cleaning
The pre-step of data processing, including find and fill or drop null values; find and combine or drop duplicated values.
- ### The goal of data cleaning
  In data consolidation, the general goal is to create a single, **authoritative record** from multiple conflicting or overlapping sources. This involves two core principles:
  **Record Prioritization:** Deciding which entire record (row) is the best representation for a unique entity (like a `date`). This usually means prioritizing data from a known, reliable source over an unknown or secondary source.
  **Feature Completion:** Once the best record is chosen, you must surgically fill in missing values ($\text{NaN}$) in specific features (columns) using the next best available data source.
- ### Check and fill na values 
  pd.isna()
  pd.fillna()
  pd.dropna(how = '', subset=['']) 
  pd.notna()
  ```
  # fillna in table df_deduped in all rows under column WESF
  df_deduped.loc[:,'WESF'].fillna(0, inplace=True)
  
  df_deduped.assign(
      TMAX=lambda x: x.TMAX.fillna(x.TMAX.median()),
      TMIN=lambda x: x.TMIN.fillna(x.TMIN.median()),
      # average of TMAX and TMIN
      TOBS=lambda x: x.TOBS.fillna((x.TMAX + x.TMIN) / 2)
  ).head()
  
  
  ```
- ### Transfer null value
  `pd.clip(lower bound, values)`
  ```
  The `clip()` method makes it possible to cap values at a specific minimum and/or maximum threshold
  ```
  `np.nan_to_num()` 
  urn `np.nan` into 0 and `-np.inf`/`np.inf` into large negative or positive finite numbers
  ```
  df_deduped.assign(
      SNWD=lambda x: np.nan_to_num(x.SNWD)
  ).head()
  ```
- ###  pd.combine\_first() 
  The $\text{.combine\_first()}$ function is a pandas method designed specifically for **index-aligned gap-filling** between two Series or DataFrames.
	- **Mechanism:** It takes the primary Series (the one the method is called on) and uses the secondary Series (the argument) **only** to fill in the spots where the primary Series has a missing value ($\text{NaN}$).
	- **Index Alignment:** This is the most critical feature. It uses the index labels (like our `date` index) to match up the rows, ensuring that the fallback data is assigned to the correct record, regardless of the row position.
	- **Prioritization Rule:** The value in the **primary Series always wins** if it is not $\text{NaN}$.
	- **Takes the  union of the two indices**. It creates a new object (a Series, in this case) that is large enough to contain every unique index label from both the primary source and the secondary source.
	- #### The Two-Stage Strategy Explained
	  
	  In the code we discussed, the cleaning process is split into two distinct, high-priority stages:
		- #### 1. Stage 1: Establishing the Primary Record (The Best Context)
		  
		  The goal here is to select the most reliable *entire row* for a given date, preserving the trustworthy data in non-$\text{WESF}$ columns.
		  
		  | **Step** | **Code** | **Mechanism of Action** | **Why We Sort First (Your Insight)** |
		  | ---- | ---- | ---- |
		  | **Sort** | `df.sort_values('station', ascending=False)` | Sorts the entire DataFrame so that the valid station IDs (e.g., 'A', 'B', 'X') are placed *before* the unknown station ID ('?'). | We sort first because the sorting imposes the desired **order of preference**. |
		  | **Drop Duplicates** | `df.drop_duplicates('date', keep='first')` | **Mechanism:** It iterates through the sorted DataFrame, looks at the unique values in the `date` column, and keeps only the *first* occurrence it finds for each date. | Because of the preceding sort, the record kept is guaranteed to be the one from the **valid station**, which we treat as the primary, authoritative record. |
			- #### 2. Stage 2: Feature Completion (The  $\text{WESF}$  Fallback)
			  
			  This stage surgically applies the supplemental data only to the column that needs it.
			  
			  The line of code `WESF=lambda x: x.WESF.combine_first(station_qm_wesf)` tells pandas: 
			  
			  1. Take the $\text{WESF}$ column from our primary deduplicated DataFrame (`x.WESF`),  
			  
			  2. Use the values from the supplemental `'?'` station data (`station_qm_wesf`) 
			        **only** when `x.WESF` is $\text{NaN}$.
			  
			  
			  This guarantees that:
				- If the valid station collected a $\text{WESF}$ value (e.g., 85), that value is kept.
				- If the valid station record has a missing $\text{WESF}$ ($\text{NaN}$), but the `'?'` station has a value (e.g., 90), the $\text{NaN}$ is filled with the 90.
- ### pd.interpolate()
  Make up some data to fill in the gap. **All imputation changes the distribution**, but $\text{.interpolate()}$ aims to minimize distortion by following the data's inherent patterns
  
  interpolation with the `interpolate()` method. We specify the `method` parameter with the interpolation strategy to use. There are many options, but we will stick with the default of `'linear'`, which will treat values as evenly spaced and place missing values in the middle of existing ones.
  
  ```
  df_deduped\
      .reindex(pd.date_range('2018-01-01', '2018-12-31', freq='D'))\
      .apply(lambda x: x.interpolate())\
      .head(10)
  
  df.reindex(pd.range(time1, time2, freq='D'). apply(lambda x: x.interpolate)
  ```